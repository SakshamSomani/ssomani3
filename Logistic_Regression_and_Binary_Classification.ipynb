{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi_wIVzzbJIU"
      },
      "source": [
        "# **Machine Learning and Statistical Methods**\n",
        "## Logistic Regression and Binary Classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs-KJZ7jbJIh"
      },
      "source": [
        "## Import Libraries\n",
        "* See various conventions and acronyms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wAKMrSQubJIh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dINsxbBvbJIi"
      },
      "source": [
        "## Load the data into a DataFrame\n",
        "* Read directly from a csv (excel-like) data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxTBFShOc9xp",
        "outputId": "3aaab03d-80b4-4a1a-8460-d86ee483877c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
            "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
            "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
            "       'Class'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "FraudDataset = pd.read_csv('/fraud.csv')\n",
        "print(type(FraudDataset))\n",
        "print(FraudDataset.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ooKj4Zwgtld"
      },
      "source": [
        "## Verify basic data statistics\n",
        "* Count the number of features. (i.e., attributes)\n",
        "* Count the number of examples. (i.e., instances and labels)\n",
        "* Unfortunately we don't know what each feature means due to privacy concerns.\n",
        "* Class variable: 0 (standard) / 1 (fradulent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ycsrrcXeYx3",
        "outputId": "acdd5c42-7da0-4cee-e2d9-9cdf4d7baadd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- # of features = 30\n",
            "- # of examples = 284807\n",
            "            Time         V1         V2        V3        V4        V5  \\\n",
            "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
            "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
            "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
            "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
            "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
            "...          ...        ...        ...       ...       ...       ...   \n",
            "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
            "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
            "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
            "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
            "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
            "\n",
            "              V6        V7        V8        V9  ...       V21       V22  \\\n",
            "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
            "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
            "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
            "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
            "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
            "...          ...       ...       ...       ...  ...       ...       ...   \n",
            "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
            "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
            "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
            "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
            "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
            "\n",
            "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
            "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
            "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
            "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
            "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
            "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
            "...          ...       ...       ...       ...       ...       ...     ...   \n",
            "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
            "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
            "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
            "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
            "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
            "\n",
            "        Class  \n",
            "0           0  \n",
            "1           0  \n",
            "2           0  \n",
            "3           0  \n",
            "4           0  \n",
            "...       ...  \n",
            "284802      0  \n",
            "284803      0  \n",
            "284804      0  \n",
            "284805      0  \n",
            "284806      0  \n",
            "\n",
            "[284807 rows x 31 columns]\n"
          ]
        }
      ],
      "source": [
        "def printBasicStats(dataset):\n",
        "  print('- # of features = %d' % (len(dataset.keys()) - 1))\n",
        "  print('- # of examples = %d' % len(dataset))\n",
        "  \n",
        "printBasicStats(FraudDataset)\n",
        "print(FraudDataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-a_Fm2du-mT"
      },
      "source": [
        "## Data inspection\n",
        "* See the label imbalance.\n",
        "* Measure the baseline accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptx29gCt6mQo",
        "outputId": "7a56d598-ddcf-4d4a-a3e0-c183b1ce23ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    284315\n",
            "1       492\n",
            "Name: Class, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "Counts = FraudDataset['Class'].value_counts()\n",
        "print(Counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaQuKmCXbJIj",
        "outputId": "69388402-5445-4755-affd-ac4a72161f79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                Time            V1            V2            V3            V4  \\\n",
            "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
            "mean    94813.859575  1.759061e-12 -8.251130e-13 -9.654937e-13  8.321385e-13   \n",
            "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
            "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
            "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
            "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
            "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
            "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
            "\n",
            "       ...           V26           V27           V28         Amount  \\\n",
            "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
            "mean   ... -5.617874e-13  3.332082e-12 -3.518874e-12      88.349619   \n",
            "std    ...  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
            "min    ... -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
            "25%    ... -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
            "50%    ... -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
            "75%    ...  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
            "max    ...  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
            "\n",
            "               Class  \n",
            "count  284807.000000  \n",
            "mean        0.001727  \n",
            "std         0.041527  \n",
            "min         0.000000  \n",
            "25%         0.000000  \n",
            "50%         0.000000  \n",
            "75%         0.000000  \n",
            "max         1.000000  \n",
            "\n",
            "[8 rows x 31 columns]\n",
            "0    284315\n",
            "1       492\n",
            "Name: Class, dtype: int64\n",
            "0.9982725143693799\n"
          ]
        }
      ],
      "source": [
        "pd.set_option('display.max_columns', 10)\n",
        "print(FraudDataset.describe(exclude=None))\n",
        "\n",
        "Counts = FraudDataset['Class'].value_counts()\n",
        "print(Counts)\n",
        "\n",
        "BaseLineAcc = Counts[0]/(Counts[0] + Counts[1])\n",
        "print(BaseLineAcc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9noHAgdBbJIk"
      },
      "source": [
        "## Data inspection Part II.\n",
        "* Measure the correlation.\n",
        "* Let's draw heatmap as an intuitive visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GIfrBGP-bJIk",
        "outputId": "f60a7cf5-d8db-4f75-a442-90997e8f1758"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Time            V1            V2            V3            V4  ...  \\\n",
            "Time    1.000000  1.173963e-01 -1.059333e-02 -4.196182e-01 -1.052602e-01  ...   \n",
            "V1      0.117396  1.000000e+00  3.777823e-12 -2.118614e-12 -1.733159e-13  ...   \n",
            "V2     -0.010593  3.777823e-12  1.000000e+00  2.325661e-12 -2.314981e-12  ...   \n",
            "V3     -0.419618 -2.118614e-12  2.325661e-12  1.000000e+00  2.046235e-13  ...   \n",
            "V4     -0.105260 -1.733159e-13 -2.314981e-12  2.046235e-13  1.000000e+00  ...   \n",
            "V5      0.173072 -3.473231e-12 -1.831952e-12 -4.032993e-12 -2.552389e-13  ...   \n",
            "V6     -0.063016 -1.306165e-13  9.438444e-13 -1.574471e-13  1.084041e-12  ...   \n",
            "V7      0.084714 -1.116494e-13  5.403436e-12  3.405586e-12  8.135064e-13  ...   \n",
            "V8     -0.036949  2.114527e-12  2.133785e-14 -1.272385e-12  7.334818e-13  ...   \n",
            "V9     -0.008660  3.016285e-14  3.238513e-13 -6.812351e-13 -7.143069e-13  ...   \n",
            "V10     0.030617 -2.615192e-12  1.463282e-12 -1.609126e-12 -1.938143e-12  ...   \n",
            "V11    -0.247689  1.866551e-12 -8.314960e-13  8.707055e-13  1.874473e-12  ...   \n",
            "V12     0.124348 -1.238745e-12  6.139448e-13 -2.730043e-12  5.393827e-13  ...   \n",
            "V13    -0.065902  7.589589e-13 -1.181068e-12 -1.020592e-12  6.813810e-13  ...   \n",
            "V14    -0.098757 -1.871054e-13 -3.384684e-13 -5.597874e-13 -1.404120e-12  ...   \n",
            "V15    -0.183453 -3.601390e-13  2.196083e-13  6.442512e-13  1.526382e-12  ...   \n",
            "V16     0.011903 -1.142884e-12 -8.000510e-13 -8.748795e-13  3.095722e-13  ...   \n",
            "V17    -0.073297  1.671073e-12  2.028957e-12 -1.058101e-12  1.151414e-14  ...   \n",
            "V18     0.090438 -5.738830e-13 -1.916566e-14 -8.846578e-13 -1.309615e-12  ...   \n",
            "V19     0.028975 -2.770259e-12 -2.237098e-13 -1.061131e-12 -9.754131e-13  ...   \n",
            "V20    -0.050866  2.662926e-13  5.839893e-13  1.873059e-12 -2.347029e-12  ...   \n",
            "V21     0.044736 -3.276238e-12  2.280202e-12  6.736294e-13 -2.696370e-12  ...   \n",
            "V22     0.144059  2.281863e-12 -2.548560e-13 -8.909339e-13  4.347776e-13  ...   \n",
            "V23     0.051142 -2.969746e-12 -4.856120e-12  4.147209e-12 -4.160969e-12  ...   \n",
            "V24    -0.016182 -1.029876e-12  6.431308e-13  3.407636e-12 -2.368743e-12  ...   \n",
            "V25    -0.233083  1.144179e-12 -9.423730e-13  5.712956e-13  1.619944e-12  ...   \n",
            "V26    -0.041407  1.835263e-12 -4.129100e-13 -2.577274e-12 -3.043100e-13  ...   \n",
            "V27    -0.005135  7.624804e-12 -9.856545e-13 -5.041444e-12 -1.456066e-12  ...   \n",
            "V28    -0.009413 -9.769215e-13  2.525513e-12  5.189109e-12 -2.832372e-12  ...   \n",
            "Amount -0.010596 -2.277087e-01 -5.314089e-01 -2.108805e-01  9.873167e-02  ...   \n",
            "Class  -0.012323 -1.013473e-01  9.128865e-02 -1.929608e-01  1.334475e-01  ...   \n",
            "\n",
            "                 V26           V27           V28    Amount     Class  \n",
            "Time   -4.140710e-02 -5.134591e-03 -9.412688e-03 -0.010596 -0.012323  \n",
            "V1      1.835263e-12  7.624804e-12 -9.769215e-13 -0.227709 -0.101347  \n",
            "V2     -4.129100e-13 -9.856545e-13  2.525513e-12 -0.531409  0.091289  \n",
            "V3     -2.577274e-12 -5.041444e-12  5.189109e-12 -0.210880 -0.192961  \n",
            "V4     -3.043100e-13 -1.456066e-12 -2.832372e-12  0.098732  0.133447  \n",
            "V5     -1.896141e-13 -2.124559e-12  1.010196e-11 -0.386356 -0.094974  \n",
            "V6      3.351239e-12  1.481307e-12 -6.069227e-13  0.215981 -0.043643  \n",
            "V7     -4.476467e-12 -1.328637e-11  2.958679e-13  0.397311 -0.187257  \n",
            "V8      1.043839e-12 -3.499804e-12  1.866598e-12 -0.103079  0.019875  \n",
            "V9     -7.723547e-13  2.428930e-12 -1.406856e-12 -0.044246 -0.097733  \n",
            "V10    -2.738577e-13  1.552492e-12  5.116568e-12 -0.101502 -0.216883  \n",
            "V11     2.718291e-12 -3.950227e-12 -4.247931e-12  0.000104  0.154876  \n",
            "V12     2.808714e-13  5.953998e-13 -7.428113e-12 -0.009542 -0.260593  \n",
            "V13    -2.008364e-12  4.975659e-12 -6.777880e-12  0.005293 -0.004570  \n",
            "V14    -3.304576e-13 -2.447674e-12 -1.700091e-12  0.033751 -0.302544  \n",
            "V15     5.478951e-13 -4.690702e-12 -4.214967e-12 -0.002986 -0.004223  \n",
            "V16    -1.323365e-12  7.022747e-12  5.737097e-13 -0.003910 -0.196539  \n",
            "V17     2.940618e-12 -1.324408e-12  1.854033e-12  0.007309 -0.326481  \n",
            "V18    -1.810692e-12 -4.949670e-12  4.113104e-12  0.035650 -0.111485  \n",
            "V19     2.412082e-12 -2.201365e-12  3.450583e-12 -0.056151  0.034783  \n",
            "V20     1.469484e-13 -2.996546e-12  6.123479e-12  0.339403  0.020090  \n",
            "V21     8.463789e-13 -8.527973e-13  4.256994e-12  0.105999  0.040413  \n",
            "V22    -1.013828e-12 -1.726653e-13  5.948423e-12 -0.064801  0.000805  \n",
            "V23    -1.002379e-12  9.199153e-12  3.819775e-12 -0.112633 -0.002685  \n",
            "V24     1.604779e-12  1.554565e-12  1.380805e-11  0.005146 -0.007221  \n",
            "V25     2.111834e-12 -6.220008e-13 -8.597190e-12 -0.047837  0.003308  \n",
            "V26     1.000000e+00  2.374854e-12 -1.036858e-11 -0.003208  0.004455  \n",
            "V27     2.374854e-12  1.000000e+00 -4.441112e-12  0.028825  0.017580  \n",
            "V28    -1.036858e-11 -4.441112e-12  1.000000e+00  0.010258  0.009536  \n",
            "Amount -3.208037e-03  2.882546e-02  1.025822e-02  1.000000  0.005632  \n",
            "Class   4.455398e-03  1.757973e-02  9.536041e-03  0.005632  1.000000  \n",
            "\n",
            "[31 rows x 31 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa063c86150>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "print(FraudDataset.corr())\n",
        "\n",
        "import seaborn as sns\n",
        "sns.heatmap(FraudDataset.corr(), cmap=sns.diverging_palette(220, 10, as_cmap=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY576FhOg7xa"
      },
      "source": [
        "## Data split\n",
        "* Must split into train and test data but with respect to the class distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ljg5JPskg7Ma"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "def splitTrainTest(df, size):\n",
        "  split = StratifiedShuffleSplit(n_splits=1, test_size=size, random_state=0)\n",
        "\n",
        "  # For each pair of train and test indices,\n",
        "  X = df.drop('Class', axis=1)\n",
        "  y = df.Class  \n",
        "  for trainIndexes, testIndexes in split.split(X, y):\n",
        "    X_train, y_train = X.iloc[trainIndexes], y.iloc[trainIndexes]\n",
        "    X_test, y_test = X.iloc[testIndexes], y.iloc[testIndexes]\n",
        "\n",
        "  return (X_train, y_train), (X_test, y_test)\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = splitTrainTest(FraudDataset, 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TecDfoLCqFDi",
        "outputId": "28a3c92d-b11b-49c7-cd86-78ac90a62762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    227451\n",
            "1       394\n",
            "Name: Class, dtype: int64\n",
            "0    56864\n",
            "1       98\n",
            "Name: Class, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(y_train.value_counts())\n",
        "print(y_test.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMcdoEH9-6Np"
      },
      "source": [
        "## Logistic Regression \n",
        "* Train the logistic regression.\n",
        "* Train again with normalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6At9tfyu_wcC",
        "outputId": "4bc4ce15-e1be-41cd-d117-f39af785a905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9992143781957032\n",
            "[[227418     33]\n",
            " [   146    248]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix \n",
        "\n",
        "def doLogisticRegression(X, y, normalize=False):\n",
        "  # If normalize option is enabled,\n",
        "  if normalize:\n",
        "    # For each feature (indexed by j as usual)\n",
        "    for j in X.columns:\n",
        "      # Subtract its column mean and update the value.\n",
        "      X[j] -= X[j].mean()\n",
        "\n",
        "      # Divide by its standard deviation and update the value.\n",
        "      X[j] /= X[j].std()\n",
        "\n",
        "  # Instanciate an object from Logistic Regression class.\n",
        "  lr = LogisticRegression()\n",
        "\n",
        "  # Perform training and prediction.\n",
        "  lr.fit(X, y)\n",
        "  y_pred = lr.predict(X)\n",
        "      \n",
        "  # Return training accuracy and confusion matrix.\n",
        "  return accuracy_score(y, y_pred), confusion_matrix(y, y_pred), lr\n",
        "\n",
        "TrainAcc, TrainConf, LR = doLogisticRegression(X_train, y_train, normalize=True)\n",
        "print(TrainAcc)\n",
        "print(TrainConf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot-CAATN20ZR",
        "outputId": "91a1da5e-d801-4dd4-a85b-ef82d1d044e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "X = [[1],[2],[3],[4]]\n",
        "y = [1,1,0,0]\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# Perform training and prediction.\n",
        "lr.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdL8uK7a20ZS",
        "outputId": "854eb40a-734f-45fa-d0b6-7d8000c9fe7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a0: [2.39571185]\n",
            "a1: [[-0.95828579]]\n",
            "a0/a1: [[2.49999725]]\n"
          ]
        }
      ],
      "source": [
        "print(f\"a0: {lr.intercept_}\")\n",
        "print(f\"a1: {lr.coef_}\")\n",
        "print(f\"a0/a1: {-lr.intercept_/lr.coef_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5LcgQphsA6n",
        "outputId": "257d6863-b882-4f44-cbb1-b81319208f7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9979986657771848\n",
            "[[56848    16]\n",
            " [   98     0]]\n"
          ]
        }
      ],
      "source": [
        "y_test_pred = LR.predict(X_test)\n",
        "TestAcc, TestConf = accuracy_score(y_test, y_test_pred), confusion_matrix(y_test, y_test_pred)\n",
        "print(TestAcc)\n",
        "print(TestConf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmzeCBoik7-b"
      },
      "source": [
        "*   Implement logistic()\n",
        "*   Implement logLikelihood()\n",
        "*   Implement predict()\n",
        "*   Implement miniBatchGradientDescent()\n",
        "*   Play with testYourCode() that compares your implementations agaisnt scikit-learn's results.\n",
        "*   Note that your log-likelihood must increase over epoch as you update the model parameter theta toward its maximum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QKm17sOc91qE"
      },
      "outputs": [],
      "source": [
        "class MyLogisticRegression:\n",
        "  # Randomly initialize the parameter vector.\n",
        "  theta = None\n",
        "\n",
        "  def logistic(self, z):\n",
        "    # Return the sigmoid function value.\n",
        "    #Complete the evaluation of logistic function given z.\n",
        "    logisticValue = 1/(1+np.exp(-z))\n",
        "    \n",
        "    return logisticValue\n",
        "\n",
        "  def logLikelihood(self, X, y):\n",
        "    # Compute the log-likelihood hood of all training examples.\n",
        "    # X: (m x (n+1)) data matrix\n",
        "    # y: (m x 1) output vector    \n",
        "\n",
        "    # If theta parameter has not trained yet,\n",
        "    if not isinstance(self.theta, np.ndarray):\n",
        "      return 0.0\n",
        "\n",
        "    # Compute the linear hypothesis given individual examples (as a whole).\n",
        "    h_theta = self.logistic(np.dot(X, self.theta))\n",
        "\n",
        "    # Evalaute the two terms in the log-likelihood.    \n",
        "    \n",
        "    #Compute the two terms in the log-likelihood of the data.\n",
        "    probability1 = np.log(h_theta)*y\n",
        "    probability0 = np.log(1-h_theta)*(1-y)\n",
        "   \n",
        "\n",
        "    # Return the average of the log-likelihood\n",
        "    m = X.shape[0]\n",
        "    return (1.0/m) * np.sum(probability1 + probability0) \n",
        "\n",
        "  def fit(self, X, y, alpha=0.01, epoch=50):\n",
        "    # Extract the data matrix and output vector as a numpy array from the data frame.\n",
        "    # Note that we append a column of 1 in the X for the intercept.\n",
        "    X = np.concatenate((np.array(X), np.ones((X.shape[0], 1), dtype=np.float64)), axis=1)\n",
        "    y = np.array(y)  \n",
        "\n",
        "    # Run mini-batch gradient descent.\n",
        "    self.miniBatchGradientDescent(X, y, alpha, epoch)\n",
        "\n",
        "  def predict(self, X):\n",
        "    # Extract the data matrix and output vector as a numpy array from the data frame.\n",
        "    # Note that we append a column of 1 in the X for the intercept.\n",
        "    X = np.concatenate((np.array(X), np.ones((X.shape[0], 1), dtype=np.float64)), axis=1)\n",
        "\n",
        "    # Perfrom a prediction only after a training happens.\n",
        "    if isinstance(self.theta, np.ndarray):\n",
        "      y_pred = self.logistic(X.dot(self.theta))\n",
        "      \n",
        "      # Given the predicted probability value, decide your class prediction 1 or 0.\n",
        "      y_pred_class = np.array([0 if a<0.5 else 1 for a in y_pred])\n",
        "      \n",
        "      return y_pred_class\n",
        "    return None\n",
        "\n",
        "  def miniBatchGradientDescent(self, X, y, alpha, epoch, batch_size=100):    \n",
        "    (m, n) = X.shape\n",
        "  \n",
        "    # Randomly initialize our parameter vector. (DO NOT CHANGE THIS PART!)\n",
        "    # Note that n here indicates (n+1) because X is already appended by the intercept term.\n",
        "    np.random.seed(2) \n",
        "    self.theta = 0.1*(np.random.rand(n) - 0.5)\n",
        "    print('L2-norm of the initial theta = %.4f' % np.linalg.norm(self.theta, 2))\n",
        "    \n",
        "    # Start iterations\n",
        "    for iter in range(epoch):\n",
        "      # Print out the progress report for every 1000 iteration.\n",
        "      if (iter % 5) == 0:\n",
        "        print('+ currently at %d epoch...' % iter)   \n",
        "        print('  - log-likelihood = %.4f' % self.logLikelihood(X, y))\n",
        "\n",
        "      # Create a list of shuffled indexes for iterating training examples.     \n",
        "      indexes = np.arange(m)\n",
        "      np.random.shuffle(indexes)\n",
        "\n",
        "      # For each mini-batch,\n",
        "      for i in range(0, m - batch_size + 1, batch_size):\n",
        "        # Extract the current batch of indexes and corresponding data and outputs.\n",
        "        indexSlice = indexes[i:i+batch_size]        \n",
        "        X_batch = X[indexSlice, :]\n",
        "        y_batch = y[indexSlice]\n",
        "\n",
        "        # For each feature\n",
        "        for j in np.arange(n):\n",
        "          \n",
        "          # Perform like a batch gradient desceint within the current mini-batch.\n",
        "          # Note that your algorithm must update self.theta[j].\n",
        "           self.theta[j] = self.theta[j] - (alpha*((self.logistic(np.dot(X_batch,self.theta))-y_batch).T.dot(X_batch[:,j]))/len(y_batch))\n",
        "        \n",
        "          \n",
        "  \n",
        "def doMyLogisticRegression(X, y, alpha, epoch, normalize=False):\n",
        "  # If normalize option is enabled,\n",
        "  if normalize:\n",
        "    # For each feature (indexed by j as usual)\n",
        "    for j in X.columns:\n",
        "      # Subtract its column mean and update the value.\n",
        "      X[j] -= X[j].mean()\n",
        "\n",
        "      # Divide by its standard deviation and update the value.\n",
        "      X[j] /= X[j].std()\n",
        "\n",
        "  # Instanciate an object from Logistic Regression class.\n",
        "  lr = MyLogisticRegression()\n",
        "\n",
        "  # Perform training and prediction.\n",
        "  lr.fit(X, y, alpha, epoch,)\n",
        "  y_pred = lr.predict(X)\n",
        "      \n",
        "  # Return training accuracy and confusion matrix.\n",
        "  return accuracy_score(y, y_pred), confusion_matrix(y, y_pred), lr\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PrBUMlklvVE",
        "outputId": "9d1e2c41-e90b-490c-f86e-329054a892e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scikit's training/test accuracies = 0.9992 / 0.9980\n",
            "Scikit's training/test confusion matrix\n",
            " [[227418     33]\n",
            " [   146    248]]\n",
            " [[56848    16]\n",
            " [   98     0]]\n",
            "[-1.66767446e-01  1.68357145e-01  2.62997387e-02  3.90900196e-02\n",
            "  9.42470156e-01  1.49724557e-01 -1.24410960e-01 -1.49150632e-01\n",
            " -2.04996491e-01 -2.87178956e-01 -8.38403196e-01 -5.15063488e-03\n",
            "  5.01854097e-02 -2.38173041e-01 -5.53811407e-01 -1.04186167e-01\n",
            " -2.22980389e-01  7.10037230e-03  7.00394427e-03  8.23866794e-02\n",
            " -3.54761664e-01  2.40133696e-01  4.12819493e-01 -5.76494059e-02\n",
            "  5.24238542e-02  2.44609344e-03 -2.80842684e-02 -2.87220019e-01\n",
            " -8.59162230e-02  2.58069755e-01 -8.63718232e+00]\n",
            "L2-norm of the initial theta = 0.1434\n",
            "+ currently at 0 epoch...\n",
            "  - log-likelihood = -0.6936\n",
            "+ currently at 5 epoch...\n",
            "  - log-likelihood = -0.0051\n",
            "+ currently at 10 epoch...\n",
            "  - log-likelihood = -0.0043\n",
            "+ currently at 15 epoch...\n",
            "  - log-likelihood = -0.0041\n",
            "+ currently at 20 epoch...\n",
            "  - log-likelihood = -0.0040\n",
            "+ currently at 25 epoch...\n",
            "  - log-likelihood = -0.0040\n",
            "+ currently at 30 epoch...\n",
            "  - log-likelihood = -0.0039\n",
            "+ currently at 35 epoch...\n",
            "  - log-likelihood = -0.0039\n",
            "+ currently at 40 epoch...\n",
            "  - log-likelihood = -0.0039\n",
            "+ currently at 45 epoch...\n",
            "  - log-likelihood = -0.0039\n",
            "+ currently at 50 epoch...\n",
            "  - log-likelihood = -0.0038\n",
            "+ currently at 55 epoch...\n",
            "  - log-likelihood = -0.0038\n",
            "+ currently at 60 epoch...\n",
            "  - log-likelihood = -0.0038\n",
            "+ currently at 65 epoch...\n",
            "  - log-likelihood = -0.0039\n",
            "+ currently at 70 epoch...\n",
            "  - log-likelihood = -0.0038\n",
            "+ currently at 75 epoch...\n",
            "  - log-likelihood = -0.0038\n",
            "+ currently at 80 epoch...\n",
            "  - log-likelihood = -0.0038\n",
            "+ currently at 85 epoch...\n",
            "  - log-likelihood = -0.0038\n",
            "+ currently at 90 epoch...\n",
            "  - log-likelihood = -0.0038\n",
            "+ currently at 95 epoch...\n",
            "  - log-likelihood = -0.0038\n",
            "My training/test accuracies = 0.9992 / 0.9978\n",
            "My training/test confusion matrix\n",
            " [[227415     36]\n",
            " [   142    252]]\n",
            " [[56836    28]\n",
            " [   98     0]]\n",
            "[-1.30514982e-01  1.56011330e-01  1.12112397e-01  5.55130445e-02\n",
            "  8.09899793e-01  1.87908037e-01 -1.42788493e-01 -1.93067608e-01\n",
            " -1.85468164e-01 -2.84448640e-01 -7.15219053e-01  2.29994833e-02\n",
            " -6.22665464e-03 -2.13489456e-01 -5.61397298e-01 -1.11100791e-01\n",
            " -2.29533522e-01  6.39001923e-03  1.61660288e-03  7.86409152e-02\n",
            " -3.46062280e-01  1.94598997e-01  3.66212143e-01 -3.09611013e-02\n",
            "  4.17779338e-02 -1.22148357e-02 -2.35247959e-02 -2.56901693e-01\n",
            " -7.87932304e-02  3.67708064e-01 -8.39003648e+00]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "def testYourCode(X_train, y_train, X_test, y_test, alpha, epoch):\n",
        "  # Test the code with scikit-learn.\n",
        "  trainAcc, trainConf, lr = doLogisticRegression(X_train, y_train, normalize=True)\n",
        "  y_test_pred = lr.predict(X_test)\n",
        "  testAcc, testConf = accuracy_score(y_test, y_test_pred), confusion_matrix(y_test, y_test_pred)\n",
        "  print(\"Scikit's training/test accuracies = %.4f / %.4f\" % (trainAcc, testAcc))\n",
        "  print(\"Scikit's training/test confusion matrix\\n %s\\n %s\" % (trainConf, testConf))\n",
        "  theta = np.append(lr.coef_[0], lr.intercept_)\n",
        "  print(theta)\n",
        "\n",
        "  # Test the code with your own version.\n",
        "  myTrainAcc, myTrainConf, myLR = doMyLogisticRegression(X_train, y_train, alpha, epoch, normalize=True)\n",
        "  my_y_test_pred = myLR.predict(X_test)\n",
        "  myTestAcc, myTestConf = accuracy_score(y_test, my_y_test_pred), confusion_matrix(y_test, my_y_test_pred)\n",
        "  print(\"My training/test accuracies = %.4f / %.4f\" % (myTrainAcc, myTestAcc))\n",
        "  print(\"My training/test confusion matrix\\n %s\\n %s\" % (myTrainConf, myTestConf))\n",
        "  print(myLR.theta)\n",
        "\n",
        "testYourCode(X_train, y_train, X_test, y_test, 0.05, 100)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}